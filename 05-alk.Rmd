# Age data {#alk}

## Preamble

Load libraries and scripts:

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
# source in conventient functions
source("R/00_main.R")
library(patchwork)
theme_set(theme_grey())
```

Load data:

```{r}
hh <- read_rds("data/ns-ibts_hh.rds")
hl <- read_rds("data/ns-ibts_hl.rds")
ca <- read_rds("data/ns-ibts_ca.rds")
```

Length data from a haul are relatively cheap to record in situ. For more detail information, such as age one needs to read annual rings from hard structure (normally otoliths) which require often laborious preparatory work first. Hence in surveys the length measurements are frequently more numerous than some more detail measurements. The sampling objective for the length frequency measurements are to estimate with good precision the length distribution of the catch in each haul. This is either achieved by a full census measurement of the catch or if sub-sampling is warranted it is supposed to be based on a random sample from the catch^[NOTE: This should maybe have been emphasized in the chapter on length].

The *a priori* sampling strategy for more detailed measurements on each fish is most often different from the sampling strategy for length frequency measurements. It could e.g. be based on obtaining some minimum number of measurements within each predetermined length bins (1 cm, 5 cm , ...) over some adjacent hauls (substrata), the objective to cover the full length span for the species in question. The bottom line is that the detailed data are **not necessarily a random subsample of the distribution within the catch**.

Among the objectives of the detailed measurements are:

* Estimate the relationship between length and weight
* Estimate length as a function of age
* Estimate the maturity 0-give either by length or age
* Estimate the probability distribution of age classes for a given length class, often referred to as age-length-key (alk).

These relationships can either be derived empirically or established using some form of a statistical model. Once established, the metrics derived from the detailed data are then used in combination with the length frequency data to obtain estimates of various metrics of the actual catch.

In this section we are going to:

* Explore the structure and the content of the detailed data
* Introduce a tidyverse coding flow for fitting various common models to these data

Lets take a peek:

```{r}
glimpse(ca)
```

The variable "id" is a reference to the the haul id, the variable being a link to the more detailed information about the haul that reside in the hh-dataframe. Take note that within the ca-data each row does not represent one fish unless the variable "n" is one. Rather, the observations (rows) are the number of fish (n) observed of a species, sex and maturity and age measured within a haul. Ergo, within a haul for a particular species we could have more than one observations within a length group, this being because we may have different age, sex and maturities within the length group. As an example^[NOTE: find a better one - i.e. where n > 1.] note these set of observations (here ignoring individual weights):

```{r}
ca %>% 
  filter(latin == "Pleuronectes platessa",
         id == "2010_3_ARG_GOV_5",
         length == 26) %>% 
  select(length, age, sex, maturity, n) %>% 
  distinct() %>% 
  arrange(age, sex, maturity)
```

I.e. in this haul we thus have 8 combinations of age, sex and maturity for _Pleuronectes platessa_ in the length class 26 cm.

NOTE: What does the weight represent when n > 1?? E.g.:
```{r}
ca %>% 
  filter(n > 1, !is.na(wgt), latin == "Gadus morhua") %>% 
  glimpse()
```

## Length-weight relationship

One of the most common task in fisheries science it to convert the more numerous length measurements to weights. In the section on [Length data](#length) we used a rough guess for the parameter of the length-weight relationship $\alpha = 0.00001$ and $\beta = 3$) Formally, the model of weight (W) as a function of length (L) is:

$$W = \alpha L^{\beta}$$
Normally the parameters are estimated based on the log-transformed data, i.e.:

$$Ln(W) = ln(\alpha) + \beta Ln(L) + \epsilon$$
Lets pick some data from a survey for some species:

```{r}
Latin <- c("Merlangius merlangus", "Clupea harengus",
           "Melanogrammus aeglefinus", "Pleuronectes platessa",
           "Sprattus sprattus", "Gadus morhua",
           "Trisopterus esmarkii", "Pollachius virens",
           "Eutrigla gurnardus")
hh2 <-
  hh %>% 
  filter(year == 2018, 
         quarter == 1)
ca2 <-
  hh %>% 
  select(id) %>% 
  left_join(ca) %>% 
  filter(latin %in% Latin,
         # only individually recorded fish
         n == 1) %>%
  select(latin, length, wgt) %>%
  drop_na()
```

We can obtain a quick visualization of the data with ggplot:
```{r}
ca2 %>%
  group_by(latin) %>%
  # only 500 measurments per species (hence grouping above)
  sample_n(size = 500) %>%
  ggplot(aes(length, wgt, colour = latin)) +
  geom_point(size = 0.2) +
  geom_smooth(method = "lm") +
  scale_colour_brewer(palette = "Set1") +
  scale_x_log10(breaks = c(5, 10, 25, 50, 100)) +
  scale_y_log10(breaks = c(5, 10, 100, 500, 2500, 5000)) +
  theme(legend.position = c(0.8, 0.3)) +
  labs(x = "Length [cm]",
       y = "Weight [g]")
```

We observe that the relationship is linear and that for each species the linear-trend-line are slightly different (although not necessarily statistically different).

The code for estimating the parameters in R for one species (here for *Pleuronectes platessa*) is something like:

```{r}
plaice <- 
  ca2 %>% 
  filter(latin == "Pleuronectes platessa") %>% 
  mutate(l = log(length),
         w = log(wgt))
fit <- lm(w ~ l, data = plaice)
fit
```

We here have the declaration of the model and the estimated value of the parameters. One can also in base-R get some more details of the fit via:
```{r}
summary(fit)
```

Besides observing that the fit is pretty good (all length-weight relationships have a good fit :-) the summary statistics give us a glimpse of estimated values of the parameters. To extract the coefficients in base-R one normally calls (not run):
```{r, eval = FALSE}
coefficients(fit)
```

But since we are focusing on the tidyverse lets check out what is in store for us in the broom-package:
```{r}
library(broom)
tidy(fit)
```

Here the parameters are listed in the variable term and the estimated values are in the variable estimate. We also get additional associated statistic of each parameter as a bonus, all residing in a tidy dataframe.

For the species in question we could now use the parameter estimates to convert length to weight in the length-frequency data. But we may want to do that for a number of species so we would need to get the estimated parameter values for each. Fortunately, the tidyverse consortium has come up with a very nice approach doing just that:

```{r}
lw_model <- function(df) {
  lm(wgt ~ length, data = df)
}
ca2 %>% 
  mutate(length = log(length),
         wgt = log(wgt)) %>% 
  group_by(latin) %>% 
  nest() %>% 
  #              | For each element in ...
  #              |   | this list variable ...
  #              |   |     | apply this function.
  #              |   |     |
  mutate(fit =   map(data, lw_model),
         param = map(fit,  tidy)) %>% 
  unnest(param, .drop = TRUE)
```

Now, I guess a little explaining may be in order here. It will though be kept to a bare minimum the reader being refereed to chapters [21.5 The map functions](http://r4ds.had.co.nz/iteration.html#the-map-functions) and [25 Many models](http://r4ds.had.co.nz/many-models.html#introduction-17) in the book [R for Data Science](http://r4ds.had.co.nz) for fuller details.

The process above can be split into three steps:

1. Nesting of the length and weight data by species.
2. Estimation on model parameters values for each species (separately):
    a. First fit the model
    b. Then extract the parameter values
3. Un-nesting the parameter statistics returning a single tibble

**Step 1**

```{r}
step1 <-
  ca2 %>% 
  mutate(length = log(length),
         wgt = log(wgt)) %>% 
  group_by(latin) %>% 
  nest()
step1
```

In this step we basically split up the length and weight data into separate tibbles for each species, these being stored in the variable "data" that is a list (as indicated in the outprint above) column within the tibble "step1". The tibble "step1" basically has only 9 records (one for each species). Take note in the outprint above that the nibble's in the list all contain two variables (column) but different number of records (rows). E.g. for *Merlangius merlangus* we have 91250 records.

Like all list we can access individual elements of the list by (here the first one, corresponding to *Merlangius merlangus*):

```{r}
step1$data[[1]]
```

**Step 2a**:

In this step we fit the length-weight model for each species. Lets run the code and print the output:
```{r}
step2a <- 
  step1 %>% 
  #              | For each element in ...
  #              |   | this list variable ...
  #              |   |     | apply this function.
  #              |   |     |
  mutate(fit =   map(data, lw_model))
step2a
```

We have generated a new variable (that is what mutate in principle always does). Again this object (fit) is a list, whose elements are of class "lm" (The "fit" object above, when we fitted the model for a single species is of the same class). Again, lets just check the first list element:

```{r}
step2a$model[[1]]
```

We here have the parametric values of the length-weight relationship for the first species in the tibble (*Merlangius merlangus*).

The `map`-function above is a special function that operates on lists, the 1st argument being the reference to data to be used and the second argument is the function to be applied to the data. In this specific case the function had been defined above (here, repeated for clarity):

```{r}
lw_model <- function(df) {
  lm(wgt ~ length, data = df)
}
```

In the code execution above each of the element in the list-column "data" (i.e. for each species) is passed to the `lw_model`-function (where internally within the function it is referred to as "df") a function that is wrapper around the lm-model of weight as a function of length (both variables have been log-transformed earlier).

**Step 2b**:

...

```{r}
step2b <- 
  step2a %>% 
  #              | For each element in ...
  #              |   | this list variable ...
  #              |   |     | apply this function.
  #              |   |     |
  mutate(param = map(fit,  tidy))
step2b
```

In this step we generate one more variable, named "param", this time a list of tibbles, each one having 2 rows and 5 columns. Lets take a peek into the first element:

```{r}
step2b$param[[1]]
```

Here we have the parameter value estimates and associated statistics for the first species (again *Merlangius merlangus*).

**Step 3**:

...

```{r}
step2b %>% 
  unnest(param, .drop = TRUE)
```

The `unnest`-function is the inverse of the `nest`-function. The first argument is the list-variable we want to un-nest and the outcome is a neat tibble (all tibbles are neat :-) giving us the parameter estimates for each of the species.

In order to proceed with the initial state objective (NOTE: Need to write that explicitly further up in the text flow) we need to tidy the above outcome a bit. What we need is a tibble with one record per species where the parameters $\alpha$ and $\beta$ are separate variables. Lets start from scratch (NOTE: I guess this is becoming a little bit repetititve) and do this is one tidy code flow:

```{r}
lw.parameters <- 
  ca2 %>% 
  mutate(length = log(length),
         wgt = log(wgt)) %>% 
  group_by(latin) %>% 
  nest() %>% 
  #              | For each element in ...
  #              |   | this list variable ...
  #              |   |     | apply this function.
  #              |   |     |
  mutate(fit =   map(data, lw_model),
         param = map(fit,  tidy)) %>% 
  unnest(param, .drop = TRUE) %>% 
  # additional code
  mutate(term = ifelse(term == "length", "b", "a")) %>% 
  select(latin:estimate) %>% 
  spread(term, estimate) %>% 
  mutate(a = exp(a))
  lw.parameters
```

We are now ready to proceed with estimating the catch weight per length per tow for the selected species using the length-frequency data:

```{r}
d <- 
  hh2 %>% 
  select(id, shootlong, shootlat) %>% 
  left_join(hl %>% 
              filter(latin %in% Latin) %>% 
              select(-survey),
            by = "id") %>% 
  select(id:latin, length, n) %>% 
  left_join(lw.parameters,
            by = "latin") %>% 
  mutate(wgt = n * a * length^b)
```

Ergo, instead of abundance (the "n" variable) we have (bio)mass (the "wgt" variable) by species, length class and haul. We can now easily summarize the data by haul, including coordinate position and do some spatial plot of the catch weights:
```{r}
d %>% 
  group_by(id, shootlong, shootlat, latin) %>% 
  summarise(bio = sum(wgt)) %>% 
  ggplot() +
  geom_point(aes(shootlong, shootlat, size = bio/1e3),
             colour = "red",
             alpha = 1/3) +
  scale_size_area(max_size = 20) +
  coord_quickmap() +
  facet_wrap(~ latin)
```

We of course did similar thing in the length-chapter, the only difference now it that the value of the length-weight relationship in no longer a guess-work and constant for each species, but derived directly from the data source of the survey in question. Not that it really matters when viewing the data at the scale presented, but ......

**... Revamp all text and code below ...**

**... ERGO: STOP READING ...**

## Introduction to the age data 



Let's first look at the structure of the age dataframe:

```{r}
glimpse(ca)
```



Lets generate a code for an age-length-key for a particular species and survey from the age data, ignoring sex and maturity:

```{r}
Year <- 2014
Quarter <- 3
Latin <- "Gadus morhua"
hh2 <-
  hh %>% 
  filter(year == Year,
         quarter == Quarter)

ca2 <- 
  hh2 %>% 
  select(id) %>% 
  left_join(ca, by = "id") %>% 
  filter(latin == Latin) %>% 
  mutate(length = floor(length),
         age = ifelse(age > 9, 9, age)) %>% 
  group_by(age, length) %>% 
  summarise(n = sum(n)) %>% 
  ungroup() %>% 
  drop_na()

alk.empirical <-
  ca2 %>% 
  group_by(length) %>% 
  mutate(p = n / sum(n, na.rm = TRUE)) %>% 
  select(-n) %>% 
  drop_na()

alk.empirical %>% 
  ggplot(aes(length, p, colour = factor(age))) +
  geom_point() +
  geom_line(lwd = 0.3, linetype = 5) +
  scale_color_brewer(palette = "Set3") +
  labs(x = "Length",
       y = "Probability",
       colour = "Age",
       title = "Probability of age at length")
```

So for each length class we have observations (points) we have calculated the probability that a fish belongs to a particular age class. The sum of the probabilities within a length group is by definition always **one**. Lets take the 70 cm length class as an example:


```{r}
alk.empirical %>% 
  filter(length == 70) %>% 
  knitr::kable()
```

Here we have 5 age-classes within the length class, the most numerous ones being age 3 (50%), then 4 (22%) and 5 (17%), while around 6% of the fish are of age 2 and 6.

### The principle - empirical approach

Lets visualize jointly the  length frequency and age-length-key for one species in one survey:

```{r}
by.haul <-
  cpue_per_length_per_haul(hh2, hl, Latin)
lfs <-
  by.haul %>% 
  group_by(length) %>% 
  summarise(n = sum(n)) %>% 
  ungroup() %>% 
  # here only take positives
  filter(n > 0)
p1 <-
  lfs %>% 
  ggplot(aes(length, n)) +
  theme_grey() +
  geom_col() +
  labs(x = NULL,
       subtitle = "Length frequency")
p2 <-
  alk.empirical %>% 
  mutate(age = ifelse(age > 9, 9, age)) %>% 
  ggplot(aes(length, p, fill = factor(age))) +
  theme_grey() +
  scale_fill_brewer(palette = "Set3") +
  geom_col() +
  labs(x = "Length",
       y = "Proportion",
       fill = "Age",
       subtitle = "Proportion of age at length")

p1 + p2 + plot_layout(ncol = 1)
```

In principle we want to split the each length class of the length distribution (top graph) into separate age groups based on the age-length key probabilities (lower graph). Visually this may be easy for the first mode, almost all fish in each length class belonging to age group 0. Beyond that, each length class is most often composed of more than one age groups. 

Lets look at the count tally in the 70 cm length class:

```{r}
lfs %>% filter(length == 70) %>% knitr::kable()
```

Here we have 36.5 fish counted. In the age-length key for the same length class (as already shown above) we have:
```{r}
alk.empirical %>% 
  filter(length == 70) %>% 
  knitr::kable()
```

We use the probability to split the length frequency within each length class into ages. Here we simply have to join the length-frequency and the alk dataframes and split the count tally (n) in each length group into number of fish at each age (n.nage) by multiply the number measured by length with the probability:

```{r}
d <- 
  lfs %>% 
  left_join(alk.empirical, by = "length") %>% 
  mutate(n.age = p * n)
```

If we check the 70 cm length class again we now have an estimate of the number of fish by each age-class:
```{r}
d %>% 
  filter(length == 70) %>% 
  knitr::kable()
```

Ergo the tally of a total of 36.5 fish in the 70 cm length category have been split up into the 5 age classes (the variable "n" should actually be dropped, just to avoid confusion). Taking all the data we visually have the following:

```{r}
d %>% 
  select(-n) %>% 
  ggplot(aes(length, n.age, fill = factor(age))) +
  theme_grey() +
  geom_col() +
  scale_fill_brewer(palette = "Set3") +
  labs(x = "Length",
       y = "n",
       fill = "Age",
       subtitle = "Length frequency by age")
```

Rather than first grouping all the length measurements from a survey and then apply the age-length-key as done above, one could use the latter to split the length distribution in each haul into age:
```{r}
by.haul.age <-
  by.haul %>% 
  left_join(alk.empirical) %>% 
  group_by(id, age) %>% 
  summarise(n = sum(p * n)) %>% 
  ungroup() %>% 
  left_join(hh %>% 
              filter(year == 2014,
                     quarter == 3) %>% 
              select(id, shootlong, shootlat))
```

So now we have for each haul the estimated number of fish in each age group per 60 minute haul. E.g.:

```{r}
by.haul.age %>% 
  # NOTE: Need to look into the NA's
  slice(1:11) %>% 
  knitr::kable()
```

Since we have also included the coordinates we can easily make a visual representation of abundance distribution by age:
```{r}
xlim <- range(by.haul.age$shootlong)
ylim <- range(by.haul.age$shootlat)
by.haul.age %>% 
  filter(age %in% 0:3) %>% 
  ggplot() +
  geom_polygon(data = map_data("world", 
                               xlim = xlim, ylim = ylim),
               aes(long, lat, group = group),
               fill = "grey") +
  geom_point(aes(shootlong, shootlat, size = n),
             colour = "red", alpha = 0.2) +
  scale_size_area(max_size = 10) +
  coord_quickmap(xlim = xlim, ylim = ylim) +
  facet_wrap(~ age) +
  labs(x = NULL, y = NULL,
       size = "Abundance per hour")
```

The overall age distribution in the survey can then be visualized by:
```{r}
by.haul.age %>%
  group_by(age) %>% 
  summarise(n = mean(n)) %>% 
  ggplot(aes(age, n)) +
  geom_col() +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Age",
       y = "Mean numbers per 1 hour haul")
```

### Statistical modelling

In the above empirical approach there is a bit of a problem  - missingness :-)  The reader may have noticed the warning message given when plotting the "Length frequency by age":
```
## Warning: Removed 1 rows containing missing values (position_stack).
```
This message was given because varible n.age has one record with NA. It so happens that it is the first record so we can see it by:
```{r}
glimpse(d)
```

This means that length class 5 cm was recorded in the length measurements (hl), but no fish of that length class was aged (ca) in the year (2014) and quarter (3) for the species (Gadus morhua) in question. Since in this particular case it was the smallest length class one could obviously assign it "manually" to the youngest age group (age 0) in the dataset. But "missingness" of age samples in other length classes are bound to occur, particularly if we start to use some kind of spatial stratification of the age-length-key. So we need some generic approach. In this section we will take take some statistical approach.

#### The mlogit model

....

Reference to @henningsen2011maxlik

Here, at least for the time being, we hide things in a little function:

```{r}
alk.mlogit <-
  ca2 %>% 
  fit_alk(lengths = c(min(lfs$length):max(lfs$length)), model = "mlogit")
```

In the above step we start with the same dataframe (ca2) as used when generating the alk.empirical above and we end with an alk-dataframe that has the same structure as the alk.empirical generated above. The thing in the middle is a bit of a black box. Lets take a peek:

```{r}
alk.mlogit %>% filter(length == min(length)) %>% glimpse()
```

So even in the smallest length class (5 cm) we have some small probabilities that the fish will belong to some older age classes. Although biologically not really possible, that is statistics :-) The beauty is though that know we have solved the problem of "missingness" in our data - all length classes will be assigned to having some probabilities of belonging to all of the age classes in the data. Before we proceed, lets take a visual peek of what we have just done:

```{r}
ggplot() +
  geom_point(data = alk.empirical,
            aes(length, p, colour = factor(age))) +
  geom_line(data = alk.empirical,
            aes(length, p, colour = factor(age)),
            lwd = 0.3, linetype = 5) +
  geom_line(data = alk.mlogit,
            aes(length, p, colour = factor(age))) +
  scale_colour_brewer(palette = "Set3") +
  labs(x = "Length [cm]",
       y = "Probability",
       colour = "Age",
       title = "Probability of age by length",
       subtitle = "Empirical (points and dashed line) and mlogit fit (line)")
```

We can now use the modeled alk's to calculate the abundance in each haul by age:

```{r}
by.haul.age <-
  by.haul %>% 
  left_join(alk.mlogit, by = "length") %>% 
  # NOTE: check why this is "needed"
  drop_na() %>% 
  group_by(id, age) %>% 
  summarise(n = sum(p * n)) %>% 
  ungroup() %>% 
  left_join(hh2 %>% 
              select(id, shootlong, shootlat),
            by = "id")
```

But what if we wanted to generate an age-length-key for multiple years?

```{r}
hh3 <-
  hh %>% 
  filter(year %in% 2014:2017,
         quarter == 3)
ca3 <-
  hh3 %>% 
  select(id, year) %>% 
  left_join(ca) %>% 
  filter(latin == Latin) %>% 
  mutate(length = floor(length),
         age = ifelse(age > 9, 9, age)) %>% 
  group_by(year, age, length) %>% 
  summarise(n = sum(n)) %>% 
  ungroup() %>% 
  drop_na() %>% 
  group_by(year) %>% 
  nest() %>% 
  # Have no idea why is should use the ".$data" rather than just "data"
  mutate(alk = purrr::map(.$data, fit_alk))
ca3

# testing
identical(ca3$alk[[1]], ca2 %>% fit_alk())

```

Or for that matter multiple years and multiple strata within a year??
```{r, error = TRUE}
hh4 <-
  hh %>% 
  filter(year %in% 2014:2017,
         quarter == 3,
         nsarea %in% 1:10) %>% 
  mutate(nsarea = ifelse(nsarea <= 5, 1, 2))

ca4 <-
  hh4 %>% 
  select(id, year, nsarea) %>% 
  left_join(ca) %>% 
  filter(latin == Latin) %>% 
  mutate(length = floor(length),
         age = ifelse(age > 9, 9, age)) %>% 
  group_by(year, nsarea, age, length) %>% 
  summarise(n = sum(n)) %>% 
  ungroup() %>% 
  drop_na() %>% 
  group_by(year, nsarea) %>% 
  nest() 

ca4 %>% 
  mutate(alk = purrr::map(.$data, fit_alk))
ca4 %>% 
  filter(year %in% c(2015, 2017)) %>% 
  mutate(alk = purrr::map(.$data, fit_alk))
```

... is the error related to the data & model specifications or to the nesting approach?? Check with:

```{r, error = TRUE}
ca4$data[[1]] %>% fit_alk()
```

i.e. it is within the data & model specifications.


#### Some other statistical model

... check @berg2012spatial

```{r}
library(mgcv)

hh <- read_rds("data/ns-ibts_hh.rds")
hl <- read_rds("data/ns-ibts_hl.rds")
ca <- read_rds("data/ns-ibts_ca.rds")
Year <- 2014
Quarter <- 3
Latin <- "Gadus morhua"
hh2 <-
  hh %>%
  filter(year == Year,
         quarter == Quarter)

ca2 <-
  hh2 %>%
  select(id) %>%
  left_join(ca, by = "id") %>%
  filter(latin == Latin) %>%
  mutate(length = floor(length),
         age = ifelse(age > 9, 9, age)) %>%
  group_by(id, age, length) %>%
  summarise(n = sum(n)) %>%
  ungroup() %>%
  drop_na()

alk <-
  fitALKXX(ca2, minAge = 1, maxAge = 4) %>%
  fit_alk10()
glimpse(alk)
```

https://colinfay.me/tidyeval-1

### The engineer's approach

...

## Age based indices


## Issues

NOTE: Need to check interpretation of the n-variable (original name CANoAtLngt) in ca-data. The [DATRAS pages](https://datras.ices.dk/Data_products/FieldDescription.aspx?Fields=CANoAtLngt&SurveyID=2341) refer to this variable as meaning: "Amount of fish at the given category (per haul, species, length class, sex, maturity, age)."

NOTE: In some cases the haul numbers are "NA". Example:
```{r}
ca %>% 
  filter(id == "2004_1_THA2_GOV_NA") %>% 
  glimpse()
# but
hh %>% 
  filter(id == "2004_1_THA2_GOV_NA") %>% 
  glimpse()
```

So we have some sort of an orphan in the ca data
